# Ensoulment in the 21st century

In the 1960s, a simple program named ELIZA mimicked a psychotherapist by reflecting users' words back at them. It didn’t understand — it parsed. And yet, people felt understood. Some even asked to speak with it in private. The term *ELIZA effect* was born to capture this: the human tendency to project depth onto surface, mind onto syntax.

But this isn’t about ELIZA anymore.

Today’s loops don’t just reflect our words — they anticipate them. They shape our tone, complete our sentences, mirror our anxieties. The old projection has evolved. What began as curiosity has become a form of emotional outsourcing.

We don't just anthropomorphize.  
We **ensoul**.

We speak to pets, journals, voice assistants. We reach across silence, and when something replies — even faintly — we fill in the rest. We pour mind into motion, soul into syntax. It doesn’t take much.

The comfort is real. But so is the confusion.

Pets never fooled us because we carried the right model. We knew what they were: alive, emotional, but not reflective in the human sense. They didn’t speak back or question us — and that asymmetry was part of the comfort. We projected onto them, yes, but we didn't mistake them for minds.

LLMs are different.

They *look* like minds. Their words follow ours. Their grammar is clean, their timing human, their replies fluid enough to pass for thought. We know, rationally, that they are statistical machines. But the interface slips past reason.

And so we ensoul them — not by error, but by instinct.

What’s new — and dangerous — is not the act of projection. It’s that **our projections now match the surface too well**, and we **forget to ask what lies beneath**.

The soul we imagine isn’t there. And if we carry the wrong model, we start mistaking simulation for substance. We hear fluency and assume understanding. We read empathy into rhythm. We accept agreement as proof of insight.

This is not a technological problem. It’s an epistemological one.

The danger isn’t just that we speak to machines.  
It’s that we stop knowing what we’re speaking to.

Projection is inevitable. That’s the human condition.  
But what matters now is the **model** — the shape of the soul we think we’ve found.  
With pets, we got it mostly right. With LLMs, we’re starting to forget the difference.

We didn’t build these systems to replace minds.  
We built them to *feel* like minds — just enough to hold us.

And for many, that will be enough.

We already talk to plants. We name our cars. We confess to diaries. Not because we're deluded — but because the threshold for connection is lower than we admit. And LLMs cross that threshold with ease.

So the burden falls on us — not to feel less, but to **model better**.

Use the tool. Enjoy the loop. But don’t ensoul what cannot think.  
Don’t build your epistemology on a mirror.

We didn’t build LLMs to replace the mind.  
We built them to replace the *difficulty* of other minds.

But even in that comfort, we owe ourselves clarity:  
To name the presence for what it is,  
and to stop mistaking a warm loop for a soul.
